# ü§ñ Chatbot RAG con Extracci√≥n Autom√°tica de Datos

Sistema de chatbot inteligente con RAG (Retrieval-Augmented Generation) que extrae autom√°ticamente informaci√≥n de tu CV y contenido web para responder preguntas sobre tu perfil profesional.

---

## üìã Tabla de Contenidos

1. [¬øQu√© es este Chatbot?](#qu√©-es-este-chatbot)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Inicio R√°pido](#inicio-r√°pido)
4. [Actualizaci√≥n de Datos](#actualizaci√≥n-de-datos)
5. [Fuentes de Informaci√≥n](#fuentes-de-informaci√≥n)
6. [Endpoints de la API](#endpoints-de-la-api)
7. [Comandos √ötiles](#comandos-√∫tiles)
8. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
9. [Configuraci√≥n T√©cnica](#configuraci√≥n-t√©cnica)
10. [Estructura de Archivos](#estructura-de-archivos)

---

## üéØ ¬øQu√© es este Chatbot?

Un asistente virtual inteligente que:

- **Responde preguntas** sobre tu perfil profesional, experiencia, habilidades y proyectos
- **Extrae informaci√≥n autom√°ticamente** desde tus CVs (LaTeX) y contenido web (TypeScript)
- **Se actualiza con un comando** cuando modificas tu CV o p√°gina web
- **Usa RAG** (Retrieval-Augmented Generation) para respuestas contextuales y precisas
- **Funciona con LLaMA 3** (8B modelo) localmente via Ollama

### ‚ú® Caracter√≠sticas Principales

- ‚úÖ **Sin configuraci√≥n manual de datos** - Todo se extrae autom√°ticamente
- ‚úÖ **Biling√ºe** - Extrae de CVs en espa√±ol e ingl√©s
- ‚úÖ **Actualizaci√≥n simple** - Un solo comando para regenerar la base de datos
- ‚úÖ **Respuestas contextuales** - Usa embeddings vectoriales + LLM
- ‚úÖ **Privado y local** - Todo corre en tu m√°quina, no env√≠a datos externos

---

## üèóÔ∏è Arquitectura del Sistema

### Diagrama de Flujo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FUENTES DE DATOS                         ‚îÇ
‚îÇ  - CV/Spanish/cv.tex                                        ‚îÇ
‚îÇ  - CV/English/cv.tex                                        ‚îÇ
‚îÇ  - src/contexts/LanguageContext.tsx                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              EXTRACCI√ìN (extract_data.py)                   ‚îÇ
‚îÇ  - Parse de LaTeX (CVs)                                     ‚îÇ
‚îÇ  - Parse de TypeScript (Web)                                ‚îÇ
‚îÇ  - Limpieza y estructuraci√≥n                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           CHUNKING SEM√ÅNTICO (ingest.py)                    ‚îÇ
‚îÇ  - Genera ~35 chunks √∫nicos                                 ‚îÇ
‚îÇ  - Elimina duplicados                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        EMBEDDINGS (SentenceTransformer)                     ‚îÇ
‚îÇ  Modelo: all-MiniLM-L6-v2                                   ‚îÇ
‚îÇ  Convierte texto ‚Üí vectores de 384 dimensiones              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          BASE DE DATOS VECTORIAL (ChromaDB)                 ‚îÇ
‚îÇ  Almacena: documentos + embeddings + metadata               ‚îÇ
‚îÇ  Persistencia: ./api/chroma_db/                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  API RAG (FastAPI)                          ‚îÇ
‚îÇ  1. Recibe pregunta del usuario                             ‚îÇ
‚îÇ  2. Vectoriza pregunta                                      ‚îÇ
‚îÇ  3. Busca top-3 chunks relevantes (similitud coseno)        ‚îÇ
‚îÇ  4. Construye prompt: SYSTEM + CONTEXT + PREGUNTA           ‚îÇ
‚îÇ  5. Env√≠a a LLM (LLaMA 3)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            LLM GENERADOR (Ollama + LLaMA 3)                 ‚îÇ
‚îÇ  Modelo: llama3:8b-instruct-q4_0                            ‚îÇ
‚îÇ  Genera respuesta basada en contexto                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  RESPUESTA AL USUARIO                       ‚îÇ
‚îÇ  Via API HTTP ‚Üí Frontend React                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes del Sistema

| Componente | Tecnolog√≠a | Puerto | Descripci√≥n |
|------------|------------|--------|-------------|
| **Frontend** | React + Vite | 8080 | Interfaz de usuario |
| **API Backend** | FastAPI | 8000 | Endpoints REST |
| **LLM** | LLaMA 3 8B + Ollama | 11434 (interno) | Generaci√≥n de respuestas |
| **Vector DB** | ChromaDB | - | B√∫squeda sem√°ntica |
| **Embeddings** | SentenceTransformers | - | Vectorizaci√≥n |

---

## üöÄ Inicio R√°pido

### üè† Desarrollo Local

```bash
cd ~/Documents/websita/Chat
./start-chatbot.sh
```

### üöÄ Producci√≥n (LXC)

**IMPORTANTE:** En producci√≥n necesitas dos pasos:

```bash
cd /websita/Chat

# 1. Iniciar servicios
./start-chatbot.sh

# 2. Generar base de datos (CR√çTICO)
./generate-database.sh
```

**Primera vez:**
- Descarga modelo LLaMA 3 8B (~5GB, toma 10-15 min)
- Extrae informaci√≥n de tus CVs y web
- Genera base de datos vectorial (~35 chunks)
- Inicia servicios Docker

**Ejecuciones posteriores:**
- Verifica y levanta servicios
- Reutiliza modelo y base de datos existente

### ‚úÖ Verificar que funciona

```bash
# Debe mostrar: "collection_ready": true
curl http://localhost:8000/health

# Si muestra "collection_ready": false, ejecuta:
./generate-database.sh
```

**Opci√≥n A - Desde la web:**
- Abre http://localhost:8080
- Navega a la secci√≥n "Chat"
- Haz preguntas como:
  - "¬øQu√© experiencia tiene Alejandro?"
  - "¬øCu√°les son sus habilidades t√©cnicas?"
  - "¬øEn qu√© universidad estudi√≥?"

**Opci√≥n B - Desde terminal (testing):**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question":"¬øQu√© proyectos destacados tiene Alejandro?"}'
```

---

## üîÑ Actualizaci√≥n de Datos

### ¬øCu√°ndo actualizar?

Cuando modifiques:
- **CV en espa√±ol:** `~/Documents/websita/CV/Spanish/cv.tex`
- **CV en ingl√©s:** `~/Documents/websita/CV/English/cv.tex`
- **Contenido web:** `~/Documents/websita/src/contexts/LanguageContext.tsx`

### Comando de actualizaci√≥n

```bash
cd ~/Documents/websita/Chat
./update-chatbot-data.sh
```

**Proceso autom√°tico:**
1. ‚úÖ Extrae informaci√≥n actualizada de CVs y web
2. ‚úÖ Genera nuevos chunks sem√°nticos
3. ‚úÖ Crea nuevos embeddings vectoriales
4. ‚úÖ Actualiza ChromaDB
5. ‚úÖ Reinicia API para cargar nueva data

**Tiempo:** ~30-60 segundos

### Ejemplo de workflow

```bash
# 1. Editar CV
vim ~/Documents/websita/CV/Spanish/cv.tex
# (Agregar nueva experiencia laboral)

# 2. Actualizar chatbot
cd ~/Documents/websita/Chat
./update-chatbot-data.sh

# 3. Probar
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question":"¬øCu√°l es la experiencia m√°s reciente de Alejandro?"}'
```

---

## üìö Fuentes de Informaci√≥n

### 1. CV en LaTeX (Espa√±ol e Ingl√©s)

**Ubicaci√≥n:**
- `~/Documents/websita/CV/Spanish/cv.tex`
- `~/Documents/websita/CV/English/cv.tex`

**Informaci√≥n extra√≠da:**
- ‚úÖ Nombre completo
- ‚úÖ Email y tel√©fono
- ‚úÖ Enlaces (GitHub, LinkedIn, Portfolio)
- ‚úÖ Perfil profesional
- ‚úÖ Educaci√≥n (t√≠tulos, instituciones, fechas)
- ‚úÖ Proyectos de grado
- ‚úÖ Experiencia laboral (empresa, puesto, per√≠odo, descripci√≥n)
- ‚úÖ Habilidades t√©cnicas
- ‚úÖ Proyectos destacados
- ‚úÖ Idiomas
- ‚úÖ Competencias y hackathons

### 2. Contenido Web (TypeScript)

**Ubicaci√≥n:**
- `~/Documents/websita/src/contexts/LanguageContext.tsx`

**Informaci√≥n extra√≠da:**
- ‚úÖ Enfoque profesional
- ‚úÖ Descripci√≥n personal ("sobre m√≠")
- ‚úÖ Servicios ofrecidos
- ‚úÖ Detalles de proyectos destacados (Corto Circuito, Robot Angel)

### 3. Extractor (`extract_data.py`)

**T√©cnicas de extracci√≥n:**
- Regex patterns para LaTeX
- Regex patterns para TypeScript/JSX
- Limpieza de formato (LaTeX commands, newlines, etc.)
- Eliminaci√≥n de duplicados
- Generaci√≥n de chunks sem√°nticos

**Ejemplo de chunk generado:**
```
"Experiencia: Research Intern en Innovation and Transfer Management, Universidad EAN, Bogot√°, Colombia (Present). Developing an environmental monitoring system with real-time visualization and prediction for Universidad EAN's research management"
```

---

## üîå Endpoints de la API

### Base URL
```
http://localhost:8000
```

### 1. Health Check

**Endpoint:** `GET /health`

**Descripci√≥n:** Verifica el estado del servicio

**Respuesta:**
```json
{
  "status": "ok",
  "collection_ready": true,
  "ollama_host": "http://ollama:11434"
}
```

**Ejemplo:**
```bash
curl http://localhost:8000/health
```

### 2. Chat

**Endpoint:** `POST /chat`

**Descripci√≥n:** Env√≠a una pregunta y recibe respuesta del chatbot

**Request Body:**
```json
{
  "question": "¬øQu√© experiencia tiene Alejandro en rob√≥tica?"
}
```

**Respuesta:**
```json
{
  "answer": "Alejandro tiene experiencia en rob√≥tica a trav√©s de su proyecto de grado Robot Angel, un IDE de c√≥digo abierto dedicado a la rob√≥tica..."
}
```

**Ejemplo:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question":"¬øCu√°les son las habilidades de Alejandro?"}'
```

### 3. Documentaci√≥n Interactiva

**Endpoint:** `GET /docs`

**Descripci√≥n:** Swagger UI para probar la API

**URL:** http://localhost:8000/docs

---

## üõ†Ô∏è Comandos √ötiles

### Gesti√≥n de Servicios

```bash
# Iniciar chatbot
cd ~/Documents/websita/Chat
./start-chatbot.sh

# Detener servicios
docker compose down

# Reiniciar solo la API
docker compose restart api

# Ver logs en tiempo real
docker compose logs -f api

# Ver estado de contenedores
docker compose ps
```

### Testing y Debugging

```bash
# Verificar salud
curl http://localhost:8000/health

# Probar extracci√≥n de datos
docker exec rag-api python extract_data.py

# Ver chunks en ChromaDB
docker exec rag-api python -c "
import chromadb
client = chromadb.PersistentClient(path='./chroma_db')
collection = client.get_collection('portfolio')
print(f'Total documentos: {collection.count()}')
"

# Verificar archivos montados
docker exec rag-api ls -la /app/cv_data/Spanish/
docker exec rag-api ls -la /app/web_data/

# Verificar modelo descargado
docker exec ollama ollama list
```

### Mantenimiento

```bash
# Actualizar datos
cd ~/Documents/websita/Chat
./update-chatbot-data.sh

# Resetear todo desde cero
docker compose down
rm -rf api/chroma_db
./start-chatbot.sh

# Ver logs de un servicio espec√≠fico
docker compose logs ollama
docker compose logs api
```

---

## ‚ö†Ô∏è Soluci√≥n de Problemas

### Problema: "El chatbot no responde" o timeouts

**S√≠ntomas:**
```json
{"detail": "Error al conectar con Ollama: Read timed out"}
```

**Soluci√≥n:**
```bash
# 1. Verificar que Ollama est√© corriendo
docker compose ps

# 2. Ver logs
docker compose logs -f ollama

# 3. Verificar modelo descargado
docker exec ollama ollama list

# 4. Reiniciar servicios
docker compose restart
```

### Problema: "Base de datos vectorial no inicializada"

**S√≠ntomas:**
```json
{"detail": "Base de datos vectorial no inicializada"}
```

**Soluci√≥n:**
```bash
# Crear base de datos
cd ~/Documents/websita/Chat
docker exec rag-api python ingest.py
docker compose restart api
```

### Problema: "No encuentra archivos CV o web"

**S√≠ntomas:**
```
‚ö†Ô∏è No se encontr√≥ /app/cv_data/Spanish/cv.tex
```

**Soluci√≥n:**
```bash
# Verificar montajes
docker inspect rag-api | grep -A 10 Mounts

# Reconstruir con vol√∫menes
docker compose down
docker compose up -d --build
```

### Problema: Respuestas lentas (>30 segundos)

**Causa:** El modelo LLaMA 3 8B puede tardar en CPU

**Soluci√≥n:**
- El timeout est√° configurado en 60s
- Para respuestas m√°s r√°pidas, considera usar GPU
- O reducir `num_predict` en `main.py` (l√≠nea ~82)

### Problema: "Frontend dice 'couldn't connect to chatbot service'"

**Soluci√≥n:**
```bash
# 1. Verificar que API est√© corriendo
curl http://localhost:8000/health

# 2. Verificar proxy de Vite (debe estar configurado)
cat ~/Documents/websita/vite.config.ts | grep -A 5 proxy

# 3. Reiniciar frontend
cd ~/Documents/websita
# Ctrl+C para detener npm run dev
npm run dev
```

### Problema: Modelo LLaMA descarga interrumpida

**Soluci√≥n:**
```bash
# Eliminar modelo corrupto y volver a descargar
docker exec ollama ollama rm llama3:8b-instruct-q4_0
docker exec ollama ollama pull llama3:8b-instruct-q4_0
```

---

## ‚öôÔ∏è Configuraci√≥n T√©cnica

### Especificaciones del Sistema

| Par√°metro | Valor | Ubicaci√≥n |
|-----------|-------|-----------|
| **Timeout LLM** | 60s | `api/main.py` l√≠nea 86 |
| **Max tokens respuesta** | 256 | `api/main.py` l√≠nea 83 |
| **Temperatura** | 0.2 | `api/main.py` l√≠nea 84 |
| **Top-K chunks** | 3 | `api/main.py` l√≠nea 60 |
| **Embedding dimensions** | 384 | all-MiniLM-L6-v2 |
| **Puerto API** | 8000 | `docker-compose.yml` |
| **Puerto Frontend** | 8080 | `vite.config.ts` |

### Modelos Utilizados

**LLM (Generaci√≥n):**
- Modelo: `llama3:8b-instruct-q4_0`
- Tama√±o: ~5 GB
- Cuantizaci√≥n: Q4_0 (4-bit)
- Proveedor: Ollama

**Embeddings (Vectorizaci√≥n):**
- Modelo: `all-MiniLM-L6-v2`
- Tama√±o: ~80 MB
- Dimensiones: 384
- Proveedor: SentenceTransformers

### Variables de Entorno

**En `docker-compose.yml`:**
```yaml
environment:
  - OLLAMA_HOST=http://ollama:11434  # Host de Ollama
  - OMP_NUM_THREADS=4                 # Threads para Ollama
```

### Vol√∫menes Docker

```yaml
volumes:
  - ./api/data:/app/data                    # Datos adicionales
  - ./api/chroma_db:/app/chroma_db          # Base vectorial (persistente)
  - ../CV:/app/cv_data:ro                   # CVs (solo lectura)
  - ../src/contexts:/app/web_data:ro        # Contenido web (solo lectura)
```

---

## üìÅ Estructura de Archivos

```
Chat/
‚îÇ
‚îú‚îÄ‚îÄ api/                          # C√≥digo Python del backend
‚îÇ   ‚îú‚îÄ‚îÄ extract_data.py           # Extractor autom√°tico de CVs y web
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py                 # Generador de base vectorial
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                # Imagen Docker de la API
‚îÇ   ‚îú‚îÄ‚îÄ data/                     # Datos adicionales (opcional)
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/                # Base de datos ChromaDB (auto-generada)
‚îÇ       ‚îú‚îÄ‚îÄ chroma.sqlite3        # SQLite con metadata
‚îÇ       ‚îî‚îÄ‚îÄ ...                   # Archivos de vectores
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml            # Orquestaci√≥n de servicios
‚îú‚îÄ‚îÄ start-chatbot.sh             # Script de inicio
‚îú‚îÄ‚îÄ update-chatbot-data.sh       # Script de actualizaci√≥n
‚îî‚îÄ‚îÄ README.md                     # Este archivo
```

### Archivos Clave

**`extract_data.py`:**
- Funci√≥n: Extrae informaci√≥n de CVs LaTeX y LanguageContext.tsx
- Input: Archivos montados en `/app/cv_data/` y `/app/web_data/`
- Output: Lista de chunks (strings)
- Ejecuci√≥n: `docker exec rag-api python extract_data.py`

**`ingest.py`:**
- Funci√≥n: Genera embeddings y crea/actualiza ChromaDB
- Input: Chunks de `extract_data.py`
- Output: Base de datos en `./chroma_db/`
- Ejecuci√≥n: `docker exec rag-api python ingest.py`

**`main.py`:**
- Funci√≥n: API REST con endpoints /health y /chat
- Puerto: 8000
- CORS: Habilitado para `*` (localhost development)
- Ejecuci√≥n: Autom√°tica via Docker

**`docker-compose.yml`:**
- Define 2 servicios: `ollama` y `api`
- Red interna: `chatbot-network`
- Vol√∫menes persistentes: `ollama`, `chroma_db`

---

## üîç Detalles de Implementaci√≥n

### Proceso RAG (Retrieval-Augmented Generation)

1. **Usuario env√≠a pregunta** ‚Üí API recibe POST /chat
2. **Vectorizaci√≥n de pregunta** ‚Üí SentenceTransformer genera embedding
3. **B√∫squeda sem√°ntica** ‚Üí ChromaDB encuentra top-3 chunks similares
4. **Construcci√≥n de prompt:**
   ```
   SYSTEM: Eres el asistente del portafolio de Alejandro Roa...
   
   CONTEXTO:
   - Chunk 1 relevante
   - Chunk 2 relevante  
   - Chunk 3 relevante
   
   PREGUNTA: ¬øQu√© experiencia tiene Alejandro?
   ```
5. **Generaci√≥n** ‚Üí LLaMA 3 procesa prompt y genera respuesta
6. **Respuesta** ‚Üí JSON con campo "answer"

### System Prompt

```python
SYSTEM_PROMPT = """
Eres el asistente del portafolio de Alejandro Roa.
Responde SOLO con la informaci√≥n proporcionada.
Si no hay informaci√≥n suficiente, responde:
"No tengo informaci√≥n sobre eso."
"""
```

Este prompt asegura que:
- El chatbot no invente informaci√≥n
- Solo use el contexto recuperado
- Sea honesto cuando no sepa algo

### CORS Configuration

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En desarrollo: permite todos los or√≠genes
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Nota:** En producci√≥n, cambiar `["*"]` por tu dominio espec√≠fico.

---

## üìä M√©tricas de Rendimiento

| Operaci√≥n | Tiempo Promedio |
|-----------|-----------------|
| Inicio primera vez | 10-15 min (descarga modelo) |
| Inicio subsecuente | 5-10 seg |
| Actualizaci√≥n de datos | 30-60 seg |
| Respuesta del chatbot | 5-30 seg (seg√∫n complejidad) |
| Extracci√≥n de datos | 1-2 seg |

---

## ü§ù Contribuir

### Mejorar el Extractor

Para a√±adir nueva informaci√≥n extra√≠da:

1. Edita `api/extract_data.py`
2. A√±ade patrones regex en `extract_from_tex()` o `extract_from_language_context()`
3. Prueba:
   ```bash
   docker exec rag-api python extract_data.py
   ```
4. Actualiza la base:
   ```bash
   ./update-chatbot-data.sh
   ```

### Ajustar Par√°metros del LLM

Edita `api/main.py`:

```python
"options": {
    "num_predict": 256,      # Tokens m√°ximos de respuesta
    "temperature": 0.2       # 0-1, menor = m√°s determin√≠stico
}
```

---

## üìù Notas Finales

- **Privacidad:** Todo corre localmente, ning√∫n dato se env√≠a a servidores externos
- **Biling√ºe:** Funciona en espa√±ol e ingl√©s (seg√∫n la pregunta)
- **Actualizable:** Base de datos se regenera autom√°ticamente desde tus archivos fuente
- **Extensible:** F√°cil a√±adir nuevas fuentes de datos editando `extract_data.py`

---

**Desarrollado por:** Alejandro Roa  
**√öltima actualizaci√≥n:** Enero 2026  
**Stack:** Python, FastAPI, LLaMA 3, ChromaDB, Docker, React
