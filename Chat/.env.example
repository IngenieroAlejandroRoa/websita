# Archivo de configuración para el chatbot RAG
# Copia este archivo como .env y ajusta según necesites

# Host de Ollama (dentro de Docker)
OLLAMA_HOST=http://ollama:11434

# Puerto de la API
API_PORT=8000

# Modelo de Ollama a usar
OLLAMA_MODEL=llama3:8b-instruct-q4_0

# Modelo de embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Número de chunks a recuperar en RAG
RAG_TOP_K=3

# Temperatura del modelo (0.0 - 1.0)
LLM_TEMPERATURE=0.2

# Tokens máximos de respuesta
MAX_TOKENS=256

# CORS Origins (separados por coma)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000,http://localhost:4173
